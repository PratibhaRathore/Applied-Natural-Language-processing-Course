{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Contents\n",
    "* [Load up dependencies](#Load-dependencies)\n",
    "* [Parsing & Structuring Data](#Parsing-&-Structuring-Data)\n",
    "* [Load the matrices and vectors from temp file](#Load-the-matrices-and-vectors)\n",
    "* [Features](#Featurization)\n",
    "* [The Model](#Building-the-model)\n",
    "* [Results](#Running-Model-on-Test-Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Parsing & Structuring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import user data from json file\n",
    "udata = []\n",
    "with open('yelp_academic_dataset_user.json') as user_data:    \n",
    "    for line in user_data:\n",
    "        temp_user = json.loads(line)\n",
    "        # remove users with less than 20 reviews\n",
    "        # users with < 20 reviews are almost certainly not elite\n",
    "        if temp_user['review_count'] >= 20:\n",
    "            # only keep useful attributes\n",
    "            temp_user = {k:temp_user[k] for k in temp_user if k in ['elite', 'review_count', 'user_id']}\n",
    "            udata.append(temp_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elite': [2005, 2006],\n",
       " 'review_count': 108,\n",
       " 'user_id': '18kPq7GPye-YQ3LyKyAZPw'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check user data (non-essential code)\n",
    "udata[0]\n",
    "\n",
    "# note: the 'elite' attribute is a list of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import review data from json file\n",
    "rdata = []\n",
    "with open('yelp_academic_dataset_review.json') as review_data:\n",
    "    for line in review_data:\n",
    "        temp_review = json.loads(line)\n",
    "        # only keep useful attributes\n",
    "        temp_review = {k:temp_review[k] for k in temp_review if k not in ['review_id', 'stars', 'type']}\n",
    "        rdata.append(temp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       " 'date': '2007-05-17',\n",
       " 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\",\n",
       " 'user_id': 'Xqd0DzHaiyRqVH3WRG7hzg',\n",
       " 'votes': {'cool': 1, 'funny': 0, 'useful': 2}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check review data (non-essential code)\n",
    "rdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import business ids from json file\n",
    "b_ids = []\n",
    "with open('yelp_academic_dataset_business.json') as business_data:\n",
    "    for line in business_data:\n",
    "        temp_biz = json.loads(line)\n",
    "        if temp_biz['state'] in ['AZ', 'NV', 'WI', 'IL', 'NC', 'PA']:    # only add businesses in the US (exclude Canada & Germany)\n",
    "            b_ids.append(temp_biz['business_id'])                        # we only need the business ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vcNAWiLM4dR7D2nwwJ7nCA'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check business ids (non-essential code)\n",
    "b_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Structuring reviews by year, then by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a dictionary for predicting elite years with\n",
    "# key: user id\n",
    "# value: [years in which their reviews let to elite status the following year]\n",
    "elite_review_years_dict = {}\n",
    "for user in udata:\n",
    "    elite_review_years_dict[user['user_id']] = [year-1 for year in user['elite']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a reviews dictionary with \n",
    "# key: (user_id, year) and \n",
    "# value: [review objects]\n",
    "reviews_dict = {}\n",
    "for review in rdata:\n",
    "    user_id = review['user_id']\n",
    "    if review['business_id'] in b_ids and user_id in elite_review_years_dict:\n",
    "        # exclude 2015 reviews since we don't know if they'll lead to elite in 2016 or not\n",
    "        year = int(review['date'][:4])\n",
    "        if year != 2015:\n",
    "            if (user_id, year) in reviews_dict:\n",
    "                reviews_dict[(user_id, year)].append(review)\n",
    "            else:\n",
    "                reviews_dict[(user_id, year)] = [review]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Sorting out the test set reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build sets of arrays such that\n",
    "# one array contains the list of a given user's review objects during a given year\n",
    "# and the other array contains an elite flag/label for that year of reviews\n",
    "# (elite = 1, non-elite = 0)\n",
    "\n",
    "# set aside 2014 reviews as the test set (roughly 20%) to see if we can predict on this most recent year after training\n",
    "\n",
    "train_dev_set = []\n",
    "train_dev_labels = []\n",
    "\n",
    "test_set = []\n",
    "test_labels = []\n",
    "\n",
    "for id_year_tuple, reviews in reviews_dict.items():\n",
    "    year = id_year_tuple[1]\n",
    "\n",
    "    if year == 2014:     \n",
    "        test_set.append(reviews)\n",
    "        test_labels.append(1) if year in elite_review_years_dict[user_id] else test_labels.append(0)                \n",
    "    else:       \n",
    "        train_dev_set.append(reviews)\n",
    "        train_dev_labels.append(1) if year in elite_review_years_dict[user_id] else train_dev_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119469\n"
     ]
    }
   ],
   "source": [
    "# sanity check: make sure the number of reviews and number of labels match for the train_dev set\n",
    "assert len(train_dev_set) == len(train_dev_labels)\n",
    "train_dev_set_len = len(train_dev_labels)\n",
    "print(train_dev_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n"
     ]
    }
   ],
   "source": [
    "# sanity check: make sure the number of reviews and number of labels match for the test set\n",
    "assert len(test_set) == len(test_labels)\n",
    "test_set_len = len(test_labels)\n",
    "print(test_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of train/dev set to test set: 0.7745706338863712\n"
     ]
    }
   ],
   "source": [
    "total_len = train_dev_set_len + test_set_len\n",
    "print(\"Proportion of train/dev set to test set: \"+str(train_dev_set_len/total_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Randomize and divide training and development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to np array to randomize/shuffle data\n",
    "np_train_dev_set = np.array(train_dev_set)\n",
    "np_train_dev_labels = np.array(train_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_indices = np.random.permutation(len(train_dev_labels))\n",
    "shuffled_train_dev_set = np_train_dev_set[rand_indices]\n",
    "shuffled_train_dev_labels = np_train_dev_labels[rand_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_length = len(shuffled_train_dev_set)\n",
    "# 80-20 between training, and dev (within training+dev)\n",
    "train_set = shuffled_train_dev_set[:subset_length*4//5]\n",
    "train_set_labels = shuffled_train_dev_labels[:subset_length*4//5]\n",
    "\n",
    "dev_set = shuffled_train_dev_set[subset_length*4//5:]\n",
    "dev_set_labels = shuffled_train_dev_labels[subset_length*4//5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that the percentage of elite users is reasonable\n",
    "def get_elite_ratio(labels):\n",
    "    elite = 0\n",
    "    for item in labels:\n",
    "        if item == 1:\n",
    "            elite += 1\n",
    "    return elite / len(labels)\n",
    "print(\"training set percentage elite: \" +str(get_elite_ratio(train_set_labels)))\n",
    "print(\"development set percentage elite: \" +str(get_elite_ratio(dev_set_labels)))\n",
    "print(\"development set percentage elite: \" +str(get_elite_ratio(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create tokenized version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dataset from: a list of lists of review objects (grouped by user-years)\n",
    "# to: a list of (list of reviews => list of sentences => list of words)\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_dataset = []\n",
    "    for review_objs in dataset:\n",
    "        tokenized_reviews = [nltk.word_tokenize(sent) for sent in sent_tokenizer.tokenize(review['text']) for review in review_objs]\n",
    "        tokenized_dataset.append(tokenized_reviews)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_train_set = tokenize_dataset(train_set)\n",
    "tokenized_dev_set   = tokenize_dataset(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Save this matrix and vector to a temp file, so info is loadable quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = open('data_sets.npz2', 'wb+')\n",
    "np.savez(data_file, train_set, train_set_labels, dev_set, dev_set_labels, tokenized_train_set, tokenized_dev_set, test_set, test_labels)\n",
    "#np.savez(data_file, train_set, train_set_labels, dev_set, dev_set_labels, test_set, test_labels)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load the matrices and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to import dependencies first\n",
    "load_data = np.load('data_sets.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_4', 'arr_2', 'arr_5', 'arr_0', 'arr_1', 'arr_3']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arrays in order are\n",
    "# train_set, train_set_labels, dev_set, dev_set_labels, tokenized_train_set, tokenized_dev_set, test_set, test_labels\n",
    "load_data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = load_data['arr_0']\n",
    "train_set_labels = load_data['arr_1']\n",
    "dev_set = load_data['arr_2']\n",
    "dev_set_labels = load_data['arr_3']\n",
    "tokenized_dataset = load_data['arr_4']\n",
    "tokenized_dataset_labels = load_data['arr_5']\n",
    "test_set = load_data['arr_6']\n",
    "test_labels = load_data['arr_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(train_set) == len(train_set_labels)\n",
    "assert len(dev_set) == len(dev_set_labels)\n",
    "assert len(tokenized_dataset) == len(tokenized_dataset_labels)\n",
    "assert len(test_set) == len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Feature development\n",
    "Adding a new feature:\n",
    "* Remember to step through each feature definition below\n",
    "* Then define the feature function, if necessary\n",
    "* Set your feature to some variable in the featurize() function\n",
    "* Make sure feature is appended in featurize(): `feature_entry.append(NEW_FEATURE)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def total_reviews(reviews_arr):\n",
    "    return len(reviews_arr)\n",
    "\n",
    "def basic_totals(reviews_arr):\n",
    "    basic_arr = []\n",
    "    \n",
    "    chars = 0\n",
    "    paragraphs = 0\n",
    "    cool_votes = 0\n",
    "    funny_votes = 0\n",
    "    useful_votes = 0\n",
    "    \n",
    "    for review in reviews_arr:\n",
    "        chars += len(review['text'])\n",
    "        paragraphs += review['text'].count(\"\\n\\n\")\n",
    "        cool_votes += review['votes']['cool']\n",
    "        funny_votes += review['votes']['funny']\n",
    "        useful_votes += review['votes']['useful']\n",
    "    \n",
    "    basic_arr.append(chars)\n",
    "    basic_arr.append(paragraphs)\n",
    "    basic_arr.append(cool_votes)\n",
    "    basic_arr.append(funny_votes)\n",
    "    basic_arr.append(useful_votes)\n",
    "    \n",
    "    return basic_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_NLP_features(tokenized_reviews):\n",
    "    NLP_features = []\n",
    "    \n",
    "    total_sents = 0\n",
    "    total_words = 0\n",
    "    vocabulary = set([])\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        for sent in review:\n",
    "            total_sents += 1\n",
    "            total_words += len(sent) - 1\n",
    "            for word in sent:\n",
    "                if word.lower() not in vocabulary:\n",
    "                    vocabulary.add(word.lower())\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    return [total_sents, total_words, vocabulary_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these features were not as successful as we hoped\n",
    "\n",
    "# average COUNT over all the user's reviews in the given year\n",
    "    # personal_pronouns: i, me, my, mine, we, us, our\n",
    "    # directed_pronouns: you, your, you're\n",
    "    # other_pronouns: he, she, him, her, they, them\n",
    "    # question_words: who, what, why, when, where, how\n",
    "# percentage versions of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # for the purpose of the calculating the readability features\n",
    "# # this code writes out each all of a user's reviews (within a year) to a single text file\n",
    "# # so the readability script can write the features to a csv file\n",
    "# # readability --csv --tokenizer='tokenizer -L en-u8 -P -S -E \"\" -N' users_train/*.txt >readability_measures_train*.csv\n",
    "\n",
    "# count = 0    # to preserve indices with labels\n",
    "# for user in train_set: \n",
    "#     data_file = open('users_train/' + '%06d' %(count) + '.txt', 'w')\n",
    "#     for review in user: \n",
    "#         data_file.write(review['text'])\n",
    "#         data_file.write('\\n\\n') # add a paragraph marker for free after each review\n",
    "#     data_file.close()\n",
    "#     count += 1\n",
    "    \n",
    "# count = 0    #to preserve order with labels\n",
    "# for user in dev_set: \n",
    "#     data_file = open('users_dev/' + '%06d' %(count) + '.txt', 'w')\n",
    "#     for review in user: \n",
    "#         data_file.write(review['text'])\n",
    "#         data_file.write('\\n\\n') # add a paragraph marker for free after each review\n",
    "#     data_file.close()\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for readability features only\n",
    "# not in use for last iteration\n",
    "\n",
    "# column indices for features\n",
    "ARI = 1\n",
    "COLMAN_LIAU = 2\n",
    "FLESCHREADINGEASE = 3\n",
    "GUNNINGFOGINDEX = 4\n",
    "KINCAID = 5\n",
    "LIX = 6\n",
    "RIX = 7\n",
    "SMOGINDEX = 8\n",
    "ARTICLE = 9\n",
    "AUXVERB = 10\n",
    "CHARACTERS = 11\n",
    "CHARACTERS_PER_WORD = 12\n",
    "COMPELX_WORDS = 13\n",
    "CONJUNCTION = 14\n",
    "INTERROGATIVE = 15\n",
    "LONG_WORDS = 16\n",
    "NOMINALIZATION = 17\n",
    "PARAGRAPHS = 18\n",
    "PREPOSITION = 19\n",
    "PRONOUN = 20\n",
    "SENTENCES = 21\n",
    "SENTENCES_PER_PARAGRAPH = 22\n",
    "SUBORDINATION = 23\n",
    "SYLL_PER_WORD = 24\n",
    "SYLLABLES = 25\n",
    "TOBEVERB = 26\n",
    "TYPE_TOKEN_RATIO = 27\n",
    "WORDS = 28\n",
    "WORDS_PER_SENTENCE = 29\n",
    "WORDTYPES = 30\n",
    "\n",
    "import csv\n",
    "featurized_matrix = []\n",
    "\n",
    "def get_readability_features():\n",
    "    # remove first column (filename)\n",
    "    # remove first row (column names header) for every file\n",
    "    for i in range(0,11): # this loop because argument list was too long; had to chop into pieces\n",
    "        f = open('readability_measures_train_dev_' + '%06d' %((i+1)*10000) + '.csv', 'r')\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if not (row[0] == ''):\n",
    "                # choose which features to add by index here:\n",
    "                featurized_matrix.append([float(x) for x in row[ARI:SMOGINDEX+1]]) \n",
    "    f = open('readability_measures_train_dev_119469.csv', 'r') # last file had a unique filename\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if not row[0] == '':\n",
    "            featurized_matrix.append([float(x) for x in row[ARI:SMOGINDEX+1]])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featurize(dataset, tokenized_dataset):\n",
    "    feature_array = []\n",
    "    index = 0\n",
    "    \n",
    "    for reviews_arr in dataset:\n",
    "        feature_entry = []\n",
    "\n",
    "        review_count = total_reviews(reviews_arr)\n",
    "        totals = basic_totals(reviews_arr)\n",
    "        \n",
    "        totals.extend(get_NLP_features(tokenized_dataset[index]))\n",
    "        \n",
    "        averages = [total/review_count for total in totals]\n",
    "\n",
    "        feature_entry.append(review_count)\n",
    "        feature_entry.extend(totals)\n",
    "        feature_entry.extend(averages)\n",
    "        \n",
    "        feature_array.append(feature_entry)\n",
    "        index += 1\n",
    "    #feature_array.extend(get_readability())\n",
    "    return feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so we can print out which features were most helpful later on\n",
    "feature_dict = {\n",
    "    1: \"total reviews\",\n",
    "    2: \"total characters\",\n",
    "    3: \"total paragraphs\",\n",
    "    4: \"total cool votes\",\n",
    "    5: \"total funny votes\",\n",
    "    6: \"total useful votes\",\n",
    "    7: \"total sentences\",\n",
    "    8: \"total words\",\n",
    "    9: \"total size of vocabulary (unique words)\",\n",
    "    10: \"chars per review\",\n",
    "    11: \"paragraphs per review\",\n",
    "    12: \"cool votes per review\",\n",
    "    13: \"funny votes per review\",\n",
    "    14: \"useful votes per review\",\n",
    "    15: \"sentences per review\",\n",
    "    16: \"words per review\",\n",
    "    17: \"size of vocabulary per review\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Apply Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "% time featurized_train = featurize(train_set, tokenized_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "% time featurized_dev = featurize(dev_set, tokenized_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenized_test_set = tokenize_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "% time featurized_test = featurize(test_set, tokenized_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "% time normalized_train = normalize(featurized_train, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 71.6 ms\n"
     ]
    }
   ],
   "source": [
    "% time normalized_dev = normalize(featurized_dev, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "% time normalized_test = normalize(featurized_test, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create subset for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_index = np.where(train_set_labels == 1)[0]\n",
    "zero_index = np.where(train_set_labels == 0)[:len(one_index)][0]\n",
    "\n",
    "subset_train = normalized_train[list(one_index)+list(zero_index)]\n",
    "subset_labels = train_set_labels[list(one_index)+list(zero_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set percentage elite: 0.23506147004969918\n",
      "development set percentage elite: 0.23492236219813334\n"
     ]
    }
   ],
   "source": [
    "# printing again for reference\n",
    "print(\"training set percentage elite: \" +str(get_elite_ratio(train_set_labels)))\n",
    "print(\"development set percentage elite: \" +str(get_elite_ratio(dev_set_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out our results with this\n",
    "def get_model_stats(dev_set, dev_set_labels, clf):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    total = 0\n",
    "    \n",
    "    for element in dev_set:\n",
    "        correct_label = dev_set_labels[total]\n",
    "        \n",
    "        if clf.predict(element) == correct_label:\n",
    "            if correct_label == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                true_pos += 1\n",
    "        else:\n",
    "            if correct_label == 0:\n",
    "                false_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        total += 1\n",
    "        \n",
    "    accuracy = (true_pos + true_neg) / total\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    print(\"T Positive: %s, F Positive: %s\" % (true_pos, false_pos))\n",
    "    print(\"F Negative: %s, T Negative: %s\" % (false_neg, true_neg))\n",
    "    print()\n",
    "    print(\"The following metrics are on a scale of 0 to 1:\")\n",
    "    print(\"Model accuracy: \"+str(accuracy))\n",
    "    print(\"Model precision: \"+str(precision))\n",
    "    print(\"Model recall: \"+str(recall))\n",
    "    print(\"Model F1 Score: \"+str(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a naive bayes model has quite low accuracy as well as precision. Its accuracy is lower than always guessing non-elite, and its precision of 29.47% means that out of the times it guesses elite, it is only correct that percentage of the time, which is definitely not great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 71.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_clf = GaussianNB()\n",
    "% time gaussian_clf.fit(normalized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 2296, F Positive: 5496\n",
      "F Negative: 3317, T Negative: 12784\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.6311471979240781\n",
      "Model precision: 0.2946611909650924\n",
      "Model recall: 0.40905041867094244\n",
      "Model F1 Score: 0.34255874673629244\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(normalized_dev, dev_set_labels, gaussian_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Non-normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 393 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_clf = GaussianNB()\n",
    "% time gaussian_clf.fit(featurized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1001, F Positive: 761\n",
      "F Negative: 4612, T Negative: 17519\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7751224207927008\n",
      "Model precision: 0.5681044267877412\n",
      "Model recall: 0.17833600570105113\n",
      "Model F1 Score: 0.2714576271186441\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(featurized_dev, dev_set_labels, gaussian_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Non-normalized Features Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An SVM-based model was prohibitivly slow to run and not particularly accurate, so we decided against using an SVM Model. The lack of wall time for the learning machine fitting was a result of us changing the featurization a little bit, which caused the machine to take over 45 minutes to run, at which point we just stopped the kernel and decided not to proceed with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()\n",
    "% time svm_clf.fit(featurized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 76.34453605658561 percent.\n",
      "Wall time: 57.9 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_accuracy(featurized_dev, dev_set_labels, svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Subset Normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 626 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf = linear_model.LogisticRegression(C=1)\n",
    "% time logreg_clf.fit(subset_train, subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a linear logistic regression model performs **very poorly**: it has an accuracy of 77.27 percent _when run on the data on which it was trained_. We know that the dev set has 0.2349 (23.49%) of its data labeled \"non-elite\", so by guessing \"non-elite\" every time, it will achieve 76.51% accuracy. As we see that it is a poor match when run on that data on which it was trained, we know then that this model must be lacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1206, F Positive: 456\n",
      "F Negative: 21260, T Negative: 72653\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7727857703374313\n",
      "Model precision: 0.7256317689530686\n",
      "Model recall: 0.05368111813406926\n",
      "Model F1 Score: 0.09996684350132626\n",
      "Wall time: 4.63 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(subset_train, subset_labels, logreg_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 396, F Positive: 270\n",
      "F Negative: 5217, T Negative: 18010\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7703511488720546\n",
      "Model precision: 0.5945945945945946\n",
      "Model recall: 0.07055050774986638\n",
      "Model F1 Score: 0.12613473483038698\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(normalized_dev, dev_set_labels, logreg_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forests Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we found that a random forests model performed the best on this data. We did not achieve an accuracy that was wildly better than baseline, but given all of the metrics we used here, it seems that, on the whole, random forests performed well for this binary classification application. Running the classifier on features without normalization seemed to perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randfor_clf = RandomForestClassifier(n_estimators=40, max_depth=5)\n",
    "% time randfor_clf.fit(normalized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1618, F Positive: 1546\n",
      "F Negative: 3995, T Negative: 16734\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7680910726991169\n",
      "Model precision: 0.511378002528445\n",
      "Model recall: 0.28825939782647425\n",
      "Model F1 Score: 0.3686908966617295\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(normalized_dev, dev_set_labels, randfor_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Non-normalized Features Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randfor_clf = RandomForestClassifier(n_estimators=40, max_depth=5)\n",
    "% time randfor_clf.fit(featurized_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 842, F Positive: 344\n",
      "F Negative: 4771, T Negative: 17936\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.7859205625078475\n",
      "Model precision: 0.7099494097807757\n",
      "Model recall: 0.15000890789239266\n",
      "Model F1 Score: 0.24768348286512723\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(featurized_dev, dev_set_labels, randfor_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy = correct predictions / total predictions\n",
    "# It is the number of correct predictions made divided by the total number of predictions made\n",
    "\n",
    "# precision = true positives / (true positives + false positives)\n",
    "# Precision can be thought of as a measure of a classifiers exactness.\n",
    "# A low precision can also indicate a large number of False Positives.\n",
    "\n",
    "# recall = true positives / (true positives + false negatives)\n",
    "# Recall can be thought of as a measure of a classifiers completeness.\n",
    "# A low recall indicates many False Negatives.\n",
    "# f1 = 2 * ((precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Running Model on Test Set \n",
    "(Non-normalized Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Positive: 1143, F Positive: 574\n",
      "F Negative: 4680, T Negative: 28373\n",
      "\n",
      "The following metrics are on a scale of 0 to 1:\n",
      "Model accuracy: 0.8488927236123095\n",
      "Model precision: 0.6656959813628421\n",
      "Model recall: 0.19629057187017002\n",
      "Model F1 Score: 0.303183023872679\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "% time get_model_stats(featurized_test, test_labels, randfor_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0033745   0.01645483  0.13675816  0.233239    0.12038536  0.07016558\n",
      "  0.00295394  0.03919613  0.03617639  0.0198593   0.09358085  0.12706954\n",
      "  0.03867808  0.01311949  0.00468971  0.02569395  0.0186052 ]\n"
     ]
    }
   ],
   "source": [
    "print(randfor_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_vals = sorted(randfor_clf.feature_importances_, reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features in order from most to least important:\n",
      "Rank: 1   |  Feature: total paragraphs       |   Importance score: 0.233239\n",
      "Rank: 2   |  Feature: total characters       |   Importance score: 0.136758\n",
      "Rank: 3   |  Feature: paragraphs per review  |   Importance score: 0.127070\n",
      "Rank: 4   |  Feature: total cool votes       |   Importance score: 0.120385\n",
      "Rank: 5   |  Feature: chars per review       |   Importance score: 0.093581\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 features in order from most to least important:\")\n",
    "index = 1\n",
    "for item in max_vals:\n",
    "    item_index = np.where(randfor_clf.feature_importances_== item)[0][0]\n",
    "    print(\"Rank: %-3d |  Feature: %-22s |   Importance score: %f\" % (index, feature_dict[item_index], item))\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "gist_id": "c42ed46055d66349747a",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
