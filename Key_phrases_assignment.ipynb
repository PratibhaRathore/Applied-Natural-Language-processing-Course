{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.collocations import *\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from urllib.request import urlopen\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "import string, random\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training on all brown sentences, excluding news corpus\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance',\n",
    "'science_fiction'])\n",
    "\n",
    "cooking_action_sents = [[('Strain', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Mix', 'VB'), ('them', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Season', 'VB'), ('them', 'PPS'), ('with', 'IN'), ('pepper', 'NN'), ('.', '.')], \n",
    "                        [('Wash', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Chop', 'VB'), ('the', 'AT'), ('greens', 'NNS'), ('.', '.')],\n",
    "                        [('Slice', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Bake', 'VB'), ('the', 'AT'), ('cake', 'NN'), ('.', '.')],\n",
    "                        [('Pour', 'VB'), ('into', 'IN'), ('a', 'AT'), ('mold', 'NN'), ('.', '.')],\n",
    "                        [('Stir', 'VB'), ('the', 'AT'), ('mixture', 'NN'), ('.', '.')],\n",
    "                        [('Moisten', 'VB'), ('the', 'AT'), ('grains', 'NNS'), ('.', '.')],\n",
    "                        [('Cook', 'VB'), ('the', 'AT'), ('duck', 'NN'), ('.', '.')],\n",
    "  \n",
    "                        [('Drain', 'VB'), ('for', 'IN'), ('one', 'CD'), ('day', 'NN'), ('.', '.')]]\n",
    "\n",
    "all_tagged_sents = cooking_action_sents + brown_tagged_sents\n",
    "all_tagged_sents\n",
    "\n",
    "def create_data_sets():\n",
    "    size = int(len(all_tagged_sents) * 0.9)\n",
    "    train_sents = all_tagged_sents[:size]\n",
    "    test_sents = all_tagged_sents[size:]\n",
    "    return train_sents, test_sents\n",
    "train_sents, test_sents = create_data_sets()\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "ngram_tagger = build_backoff_tagger(train_sents)\n",
    "\n",
    "more_stopwords = [\"''\", \"--\",\"``\", \"mr.\", \"mrs.\", \"n't\", \"'s\", \"'i\",\"said\"]\n",
    "my_stopwords = stop_words + more_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_tag_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences \n",
    "    sents = [nltk.word_tokenize(word.lower()) for word in raw_sents] # tokenize sentences\n",
    "    tagged_POS_sents = [ngram_tagger.tag(word) for word in sents] # tags sentences\n",
    "    return tagged_POS_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_normed_unigrams(tagged_sents, num_terms=50):\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "        \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation # remove punctuation\n",
    "                           and word[1].startswith('N')]  #retain only nouns \n",
    "   \n",
    "    top_normed_tagpairs = nltk.FreqDist(normed_tagged_words).most_common(num_terms) #get the num_terms most frequent\n",
    "    return [word for (word,count) in top_normed_tagpairs] #extract out the words from the pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Unigram for Brown News Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing top 50 unigrams in Brown News Corpus... \n",
      "\n",
      "['mrs.', 'year', 'state', 'mr.', 'president', 'home', 'school', 'time', 'week', 'day', 'member', 'house', 'city', 'bill', 'committee', 'service', 'government', 'program', 'county', 'game', 'month', 'man', 'university', 'company', 'law', 'car', 'board', 'tax', 'kennedy', 'john', 'night', 'meeting', 'administration', 'court', 'family', 'plan', 'library', 'club', 'sale', 'country', 'u.s.', 'problem', 'party', 'system', 'case', 'people', 'group', 'child', 'yesterday', 'cent']\n"
     ]
    }
   ],
   "source": [
    "brown_top_unigrams = freq_normed_unigrams(brown.tagged_sents(categories = 'news'))\n",
    "print('Printing top 50 unigrams in Brown News Corpus... \\n')\n",
    "print(brown_top_unigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Bigrams and the conditional Freq Dist, given most frequent Unigram- Brown News Corpus\n",
    "Finding frequent Bigrams by applying conditional freq distribution on each of the top unigram found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. , samuel , goodis , representing , today , part , project , halted , end , seven , year , students , found , mantlepiece , attributed , samuel , mcintyre , salem , special , interest , state , widely , among , stage , filial , piety , totally , devoted , friend , beaten , mr. , pezza , 734 , hartford , avenue , central , figure , prominently , new , zealand , president , nothing , abstention , u.s. , first , jane , drury , lane , already , surpassed , home , run , well , winter , tour , reached , 1-1/2-story , brick , paneled , doors , school , dance , fuhrmann's , 770 , students , junior , year , ordered , university , loath , time , reaction , bitter , winter , long , pass , throw , outside , world , competition , week , world , parolees , mother , young , ladies , waiting , misses , ticker , day , reception , daughter , rhodes , semmes , baker , houston , dr. , michael , walsh , member , nevah , sholom , congregation , house , staff , librarian-board , relationships , two , stop , lights , order , speed , chase , city , rumford , area , depending , whether , companionship , $22.50 , per , cubic , feet , bill , house , wednesday , night , playing , center , call , dresbach's , son , calls , committee , u.s. , typewriter , hand , hand , presented , young , neighbor , obtained , license , service , county , commission , planning , district , county , rapidly , expanding , share , oriole , government , thinking , jack , collins , tall , bookcases , program , socialized , medicine , physiology , discovering , genes , affect , heredity , controlling , ball , county , hamlet , 250 , people , happy , players , new , demand , sense , creative , game , kicked , several , defendants , strongly , indicated , plans , issue , million , million , month , ended , laying , ties , railroad , streamliner , city , could , gave , familiarity , man , bent , hand , presented , said , sooner , later , meantime , state , library , university , irrespective , race , soon , new , little , foreign , relations , voluntary , agencies , company , may , four , plays , eldon , moritz , listed , longhorn , roster , right , law , possible , farmers , forgetting , birds , occasionally , deserts , simple , reason , quick , car , didn't , attack , barely , put , president , clearly , playing , 15th , missing , board , sponsor , saturday , elvis , leonard , breuer , william , ball , bat , four , tax , assessors , review , united , steel , company , dancers , symphony , conductor , earl , kennedy , administration , first , time , consumed , pilot , tower , cleared , plane , slowed , john , morgan , william , keeps , town's , doings , daily , via , tribune , tells , night , our , lady , bluebird , partner , yuri , soloviev , wonderfully , virile , acrobatic , meeting , purpose , amount , may , national , audubon , convention , russian , tanks , artillery , administration , past , needs , stimulus , communication , faculty , dynamic , administration , saying , juror , court , limited , questionable , evidence , late , 1959 , made , u.s. , try , let , family , difference , first , news , frankie , fairly , glutted , ideas , based , need , plan , cover , aug. , wanted , car , patrol , north , worse , truth , though , library , leader , time , jan. , thompson , scarcely , glanced , saw , man , stuck , club , westfield , state , appeal , mantle , needs , feel , obligated , make , rural , sale , paintings , sculpture , philmont , night , club , winds , hotel , newark , essex , country , never , arrested , perjury , charge , federal , law , cannot , imagine , achievement , u.s. , textile , imports , less-developed , countries , control , programs , children's , librarian , say , problem , must , permit , free , action , week , prizes , offered , treasury , bills , party , ever , gaining , coveted , promotion , faced , new , ordinance , would , selected , system , passes , 34.7 , yards , inn , adjoining , country , small , bomb , little , case , point , added , list , neighborhood , $250,000 , it's , al's , way , watch , people , don't , believe , future , depends , return , office , cut , meet , probate , group , instead , detach , castro , dragnet , massive , assaults , independent , sources , child , one , congressional , district , voters , believe , permit , free , enterprise , merely , yesterday , mrs. , guy , lumia , especially , sam , thereupon , decided , members , taunted , cent , 64-cent , anticipated , deficit , million , million , americans , teen-agers , "
     ]
    }
   ],
   "source": [
    "\n",
    "brown = nltk.corpus.brown\n",
    "brown_sents = brown.sents(categories='news')\n",
    "\n",
    "bigram_list = []\n",
    "\n",
    "# generation function make a choice from the probable continuation words\n",
    "def generate_model(cfdist, word, num=10):\n",
    "    for i in range(num):\n",
    "        print(word, \",\" ,end=' ')\n",
    "        if cfdist[word]:\n",
    "            words = list(cfdist[word])\n",
    "            word = random.choice(words)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "\n",
    "for sent in brown_sents:\n",
    "    sent_lower = [w.lower() for w in sent if w not in nltk.corpus.stopwords.words('english') \n",
    "                  and w not in punctuation and len(w) > 2]\n",
    "    bigrams = list(nltk.bigrams(sent_lower))\n",
    "    bigram_list.extend(bigrams)\n",
    "\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigram_list)\n",
    "\n",
    "\n",
    "for word in brown_top_unigrams:\n",
    "    generate_model(cfd, word)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Unigrams with Hyperterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categories_from_hypernyms(termlist, num_terms=50):\n",
    "    \n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict(list)\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms = hypterms + [hyp.name()]      # Extract the hypernym name and add to list\n",
    "                hypterms_dict[hyp.name()].append(term)  # Extract examples and add them to dict\n",
    "                \n",
    "    hypfd = nltk.FreqDist(hypterms)\n",
    "    for (name, count) in hypfd.most_common(num_terms):\n",
    "        print (name, '({0})'.format(count))\n",
    "        print ('\\t', ', '.join(set(hypterms_dict[name])))\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING UNIGRAMS WITH HYPERNYMS FOR BROWN NEWS CORPUS..\n",
      "\n",
      "time_period.n.01 (15)\n",
      "\t year, school, day, night, week, time, month\n",
      "\n",
      "\n",
      "building.n.01 (6)\n",
      "\t house, club, library, school\n",
      "\n",
      "\n",
      "group.n.01 (5)\n",
      "\t man, people, system\n",
      "\n",
      "\n",
      "administrative_district.n.01 (5)\n",
      "\t country, county, state, city\n",
      "\n",
      "\n",
      "person.n.01 (5)\n",
      "\t man, party, child, case\n",
      "\n",
      "\n",
      "unit.n.03 (5)\n",
      "\t house, home, family, company, member\n",
      "\n",
      "\n",
      "time_unit.n.01 (4)\n",
      "\t day, month, night\n",
      "\n",
      "\n",
      "collection.n.01 (4)\n",
      "\t library, family, law\n",
      "\n",
      "\n",
      "educational_institution.n.01 (3)\n",
      "\t university, school\n",
      "\n",
      "\n",
      "room.n.01 (3)\n",
      "\t john, library, court\n",
      "\n",
      "\n",
      "body.n.02 (3)\n",
      "\t administration, university, school\n",
      "\n",
      "\n",
      "male.n.02 (3)\n",
      "\t man\n",
      "\n",
      "\n",
      "compartment.n.02 (3)\n",
      "\t car\n",
      "\n",
      "\n",
      "social_gathering.n.01 (3)\n",
      "\t party, meeting, company\n",
      "\n",
      "\n",
      "legal_document.n.01 (2)\n",
      "\t law, bill\n",
      "\n",
      "\n",
      "political_unit.n.01 (2)\n",
      "\t country, state\n",
      "\n",
      "\n",
      "game_equipment.n.01 (2)\n",
      "\t man, game\n",
      "\n",
      "\n",
      "attribute.n.02 (2)\n",
      "\t state, time\n",
      "\n",
      "\n",
      "system.n.04 (2)\n",
      "\t government, program\n",
      "\n",
      "\n",
      "association.n.01 (2)\n",
      "\t club, family\n",
      "\n",
      "\n",
      "selling.n.01 (2)\n",
      "\t sale\n",
      "\n",
      "\n",
      "social_control.n.01 (2)\n",
      "\t administration, government\n",
      "\n",
      "\n",
      "location.n.01 (2)\n",
      "\t home\n",
      "\n",
      "\n",
      "set.n.05 (2)\n",
      "\t party, company\n",
      "\n",
      "\n",
      "idea.n.01 (2)\n",
      "\t plan, program\n",
      "\n",
      "\n",
      "force.n.04 (2)\n",
      "\t service, law\n",
      "\n",
      "\n",
      "region.n.01 (2)\n",
      "\t county, house\n",
      "\n",
      "\n",
      "title.n.06 (2)\n",
      "\t mrs., mr.\n",
      "\n",
      "\n",
      "family.n.04 (2)\n",
      "\t people, house\n",
      "\n",
      "\n",
      "activity.n.01 (2)\n",
      "\t service, game\n",
      "\n",
      "\n",
      "government.n.01 (2)\n",
      "\t state, court\n",
      "\n",
      "\n",
      "organization.n.01 (2)\n",
      "\t party, company\n",
      "\n",
      "\n",
      "gathering.n.01 (2)\n",
      "\t year, meeting\n",
      "\n",
      "\n",
      "work_time.n.01 (2)\n",
      "\t week, day\n",
      "\n",
      "\n",
      "container.n.01 (2)\n",
      "\t case\n",
      "\n",
      "\n",
      "adult.n.01 (2)\n",
      "\t man, case\n",
      "\n",
      "\n",
      "head_of_state.n.01 (2)\n",
      "\t president\n",
      "\n",
      "\n",
      "concept.n.01 (2)\n",
      "\t law\n",
      "\n",
      "\n",
      "beginning.n.04 (1)\n",
      "\t home\n",
      "\n",
      "\n",
      "audience.n.01 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "region.n.03 (1)\n",
      "\t country\n",
      "\n",
      "\n",
      "people.n.01 (1)\n",
      "\t country\n",
      "\n",
      "\n",
      "document.n.01 (1)\n",
      "\t program\n",
      "\n",
      "\n",
      "case.n.01 (1)\n",
      "\t time\n",
      "\n",
      "\n",
      "question.n.02 (1)\n",
      "\t problem\n",
      "\n",
      "\n",
      "residence.n.01 (1)\n",
      "\t home\n",
      "\n",
      "\n",
      "arrangement.n.03 (1)\n",
      "\t plan\n",
      "\n",
      "\n",
      "emotional_state.n.01 (1)\n",
      "\t state\n",
      "\n",
      "\n",
      "relative.n.01 (1)\n",
      "\t family\n",
      "\n",
      "\n",
      "abstraction.n.06 (1)\n",
      "\t group\n",
      "\n",
      "\n",
      "philosophy.n.02 (1)\n",
      "\t law\n",
      "\n",
      "\n",
      "electrical_device.n.01 (1)\n",
      "\t board\n",
      "\n",
      "\n",
      "manservant.n.01 (1)\n",
      "\t man\n",
      "\n",
      "\n",
      "deference.n.01 (1)\n",
      "\t court\n",
      "\n",
      "\n",
      "sheet.n.06 (1)\n",
      "\t board\n",
      "\n",
      "\n",
      "set.n.02 (1)\n",
      "\t group\n",
      "\n",
      "\n",
      "play.n.14 (1)\n",
      "\t game\n",
      "\n",
      "\n",
      "management.n.02 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "base.n.03 (1)\n",
      "\t home\n",
      "\n",
      "\n",
      "plan_of_action.n.01 (1)\n",
      "\t system\n",
      "\n",
      "\n",
      "time.n.03 (1)\n",
      "\t day\n",
      "\n",
      "\n",
      "convergence.n.04 (1)\n",
      "\t meeting\n",
      "\n",
      "\n",
      "agreement.n.01 (1)\n",
      "\t sale\n",
      "\n",
      "\n",
      "statement.n.07 (1)\n",
      "\t bill\n",
      "\n",
      "\n",
      "court.n.01 (1)\n",
      "\t court\n",
      "\n",
      "\n",
      "framework.n.03 (1)\n",
      "\t case\n",
      "\n",
      "\n",
      "academic_administrator.n.01 (1)\n",
      "\t president\n",
      "\n",
      "\n",
      "hotel.n.01 (1)\n",
      "\t court\n",
      "\n",
      "\n",
      "erectile_organ.n.01 (1)\n",
      "\t member\n",
      "\n",
      "\n",
      "company.n.01 (1)\n",
      "\t service\n",
      "\n",
      "\n",
      "brim.n.02 (1)\n",
      "\t bill\n",
      "\n",
      "\n",
      "visitor.n.01 (1)\n",
      "\t company\n",
      "\n",
      "\n",
      "type.n.04 (1)\n",
      "\t case\n",
      "\n",
      "\n",
      "show.n.03 (1)\n",
      "\t program\n",
      "\n",
      "\n",
      "law_enforcement_agency.n.01 (1)\n",
      "\t law\n",
      "\n",
      "\n",
      "animal_group.n.01 (1)\n",
      "\t school\n",
      "\n",
      "\n",
      "fact.n.01 (1)\n",
      "\t case\n",
      "\n",
      "\n",
      "convergence.n.01 (1)\n",
      "\t meeting\n",
      "\n",
      "\n",
      "dimension.n.01 (1)\n",
      "\t time\n",
      "\n",
      "\n",
      "legislature.n.01 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "happening.n.01 (1)\n",
      "\t case\n",
      "\n",
      "\n",
      "management.n.01 (1)\n",
      "\t administration\n",
      "\n",
      "\n",
      "customer.n.01 (1)\n",
      "\t john\n",
      "\n",
      "\n",
      "tableware.n.01 (1)\n",
      "\t service\n",
      "\n",
      "\n",
      "hominid.n.01 (1)\n",
      "\t man\n",
      "\n",
      "\n",
      "work.n.01 (1)\n",
      "\t service\n",
      "\n",
      "\n",
      "geographic_point.n.01 (1)\n",
      "\t meeting\n",
      "\n",
      "\n",
      "unit.n.05 (1)\n",
      "\t group\n",
      "\n",
      "\n",
      "grammatical_category.n.01 (1)\n",
      "\t case\n",
      "\n",
      "\n",
      "taxonomic_group.n.01 (1)\n",
      "\t family\n",
      "\n",
      "\n",
      "dark.n.01 (1)\n",
      "\t night\n",
      "\n",
      "\n",
      "cortege.n.02 (1)\n",
      "\t court\n",
      "\n",
      "\n",
      "mouth.n.02 (1)\n",
      "\t bill\n",
      "\n",
      "\n",
      "religious_ceremony.n.01 (1)\n",
      "\t service\n",
      "\n",
      "\n",
      "bed_linen.n.01 (1)\n",
      "\t case\n",
      "\n",
      "\n",
      "accommodation.n.05 (1)\n",
      "\t service\n",
      "\n",
      "\n",
      "baseball_team.n.01 (1)\n",
      "\t club\n",
      "\n",
      "\n",
      "diversion.n.01 (1)\n",
      "\t game\n",
      "\n",
      "\n",
      "complement.n.03 (1)\n",
      "\t company\n",
      "\n",
      "\n",
      "tenure.n.01 (1)\n",
      "\t administration\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frequent Unigrams with Hyperterms for Brown News corpus\n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "brown_top_terms = freq_normed_unigrams(brown.tagged_sents(categories = 'news'))\n",
    "\n",
    "print(\"PRINTING UNIGRAMS WITH HYPERNYMS FOR BROWN NEWS CORPUS..\\n\")\n",
    "categories_from_hypernyms(brown_top_terms,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING UNIGRAMS WITH HYPERNYMS FOR OLIVER TWIST CORPUS...\n",
      "\n",
      "time_period.n.01 (11)\n",
      "\t life, day, time, night\n",
      "\n",
      "\n",
      "person.n.01 (7)\n",
      "\t man, jew, face, child, life, friend\n",
      "\n",
      "\n",
      "woman.n.01 (5)\n",
      "\t girl, lady\n",
      "\n",
      "\n",
      "male.n.02 (4)\n",
      "\t man, boy\n",
      "\n",
      "\n",
      "time_unit.n.01 (3)\n",
      "\t day, night\n",
      "\n",
      "\n",
      "man.n.01 (3)\n",
      "\t sir, gentleman, boy\n",
      "\n",
      "\n",
      "building.n.01 (3)\n",
      "\t house\n",
      "\n",
      "\n",
      "female.n.02 (3)\n",
      "\t girl, woman\n",
      "\n",
      "\n",
      "being.n.01 (3)\n",
      "\t life\n",
      "\n",
      "\n",
      "external_body_part.n.01 (3)\n",
      "\t face, head\n",
      "\n",
      "\n",
      "opportunity.n.01 (3)\n",
      "\t day, room, street\n",
      "\n",
      "\n",
      "communication.n.02 (2)\n",
      "\t voice\n",
      "\n",
      "\n",
      "lover.n.01 (2)\n",
      "\t dear, girl\n",
      "\n",
      "\n",
      "cognition.n.01 (2)\n",
      "\t place, head\n",
      "\n",
      "\n",
      "condition.n.01 (2)\n",
      "\t place, way\n",
      "\n",
      "\n",
      "structure.n.04 (2)\n",
      "\t head\n",
      "\n",
      "\n",
      "advocate.n.01 (2)\n",
      "\t friend, voice\n",
      "\n",
      "\n",
      "position.n.07 (2)\n",
      "\t room, way\n",
      "\n",
      "\n",
      "adult.n.01 (2)\n",
      "\t man, woman\n",
      "\n",
      "\n",
      "body_servant.n.01 (2)\n",
      "\t man, gentleman\n",
      "\n",
      "\n",
      "manservant.n.01 (2)\n",
      "\t man, gentleman\n",
      "\n",
      "\n",
      "title.n.06 (2)\n",
      "\t mrs., mr.\n",
      "\n",
      "\n",
      "play.n.08 (2)\n",
      "\t doctor, house\n",
      "\n",
      "\n",
      "point.n.02 (2)\n",
      "\t place\n",
      "\n",
      "\n",
      "thoroughfare.n.01 (2)\n",
      "\t street\n",
      "\n",
      "\n",
      "implementation.n.02 (1)\n",
      "\t way\n",
      "\n",
      "\n",
      "group.n.01 (1)\n",
      "\t man\n",
      "\n",
      "\n",
      "beginning.n.04 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "audience.n.01 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "word.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "domestic_animal.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "space.n.03 (1)\n",
      "\t place\n",
      "\n",
      "\n",
      "ability.n.02 (1)\n",
      "\t hand\n",
      "\n",
      "\n",
      "animation.n.03 (1)\n",
      "\t life\n",
      "\n",
      "\n",
      "case.n.01 (1)\n",
      "\t time\n",
      "\n",
      "\n",
      "front.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "residence.n.01 (1)\n",
      "\t place\n",
      "\n",
      "\n",
      "calendar_day.n.01 (1)\n",
      "\t day\n",
      "\n",
      "\n",
      "artifact.n.01 (1)\n",
      "\t way\n",
      "\n",
      "\n",
      "sound.n.04 (1)\n",
      "\t voice\n",
      "\n",
      "\n",
      "game_equipment.n.01 (1)\n",
      "\t man\n",
      "\n",
      "\n",
      "region.n.01 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "motivation.n.01 (1)\n",
      "\t life\n",
      "\n",
      "\n",
      "environment.n.01 (1)\n",
      "\t street\n",
      "\n",
      "\n",
      "aggressiveness.n.01 (1)\n",
      "\t face\n",
      "\n",
      "\n",
      "surface.n.02 (1)\n",
      "\t face\n",
      "\n",
      "\n",
      "line.n.05 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "tune.n.01 (1)\n",
      "\t voice\n",
      "\n",
      "\n",
      "time.n.03 (1)\n",
      "\t day\n",
      "\n",
      "\n",
      "black_man.n.01 (1)\n",
      "\t boy\n",
      "\n",
      "\n",
      "cornbread.n.01 (1)\n",
      "\t dodger\n",
      "\n",
      "\n",
      "linear_unit.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "top.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "side.n.01 (1)\n",
      "\t hand\n",
      "\n",
      "\n",
      "share.n.01 (1)\n",
      "\t way\n",
      "\n",
      "\n",
      "area.n.01 (1)\n",
      "\t eye\n",
      "\n",
      "\n",
      "room.n.01 (1)\n",
      "\t door\n",
      "\n",
      "\n",
      "striker.n.05 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "scholar.n.01 (1)\n",
      "\t doctor\n",
      "\n",
      "\n",
      "type.n.04 (1)\n",
      "\t face\n",
      "\n",
      "\n",
      "area.n.06 (1)\n",
      "\t place\n",
      "\n",
      "\n",
      "formation.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "innocent.n.01 (1)\n",
      "\t dear\n",
      "\n",
      "\n",
      "dimension.n.01 (1)\n",
      "\t time\n",
      "\n",
      "\n",
      "individual.n.02 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "attribute.n.02 (1)\n",
      "\t time\n",
      "\n",
      "\n",
      "leader.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "sacred_text.n.01 (1)\n",
      "\t word\n",
      "\n",
      "\n",
      "collection.n.01 (1)\n",
      "\t hand\n",
      "\n",
      "\n",
      "hominid.n.01 (1)\n",
      "\t man\n",
      "\n",
      "\n",
      "front.n.04 (1)\n",
      "\t face\n",
      "\n",
      "\n",
      "means.n.01 (1)\n",
      "\t voice\n",
      "\n",
      "\n",
      "sense_organ.n.01 (1)\n",
      "\t eye\n",
      "\n",
      "\n",
      "situation.n.02 (1)\n",
      "\t place\n",
      "\n",
      "\n",
      "foam.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "order.n.01 (1)\n",
      "\t word\n",
      "\n",
      "\n",
      "pressure.n.01 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "applause.n.01 (1)\n",
      "\t hand\n",
      "\n",
      "\n",
      "expression.n.03 (1)\n",
      "\t voice\n",
      "\n",
      "\n",
      "journey.n.01 (1)\n",
      "\t way\n",
      "\n",
      "\n",
      "prison_term.n.01 (1)\n",
      "\t life\n",
      "\n",
      "\n",
      "coil.n.06 (1)\n",
      "\t head\n",
      "\n",
      "\n",
      "choice.n.01 (1)\n",
      "\t way\n",
      "\n",
      "\n",
      "card_player.n.01 (1)\n",
      "\t hand\n",
      "\n",
      "\n",
      "cleaner.n.03 (1)\n",
      "\t woman\n",
      "\n",
      "\n",
      "vicinity.n.01 (1)\n",
      "\t place\n",
      "\n",
      "\n",
      "religious.n.01 (1)\n",
      "\t monk\n",
      "\n",
      "\n",
      "attention.n.01 (1)\n",
      "\t eye\n",
      "\n",
      "\n",
      "positive_identification.n.01 (1)\n",
      "\t word\n",
      "\n",
      "\n",
      "structure.n.01 (1)\n",
      "\t door\n",
      "\n",
      "\n",
      "deceiver.n.01 (1)\n",
      "\t dodger\n",
      "\n",
      "\n",
      "grammatical_relation.n.01 (1)\n",
      "\t voice\n",
      "\n",
      "\n",
      "management.n.02 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "legislature.n.01 (1)\n",
      "\t house\n",
      "\n",
      "\n",
      "countenance.n.01 (1)\n",
      "\t face\n",
      "\n",
      "\n",
      "sidereal_time.n.01 (1)\n",
      "\t day\n",
      "\n",
      "\n",
      "vertical_surface.n.01 (1)\n",
      "\t face\n",
      "\n",
      "\n",
      "geographical_area.n.01 (1)\n",
      "\t place\n",
      "\n",
      "\n",
      "descendant.n.01 (1)\n",
      "\t child\n",
      "\n",
      "\n",
      "forepaw.n.01 (1)\n",
      "\t hand\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Frequent Unigrams with Hyperterms for Oliver Twist corpus\n",
    "\n",
    "with open('Oliver_Copy.txt', 'r') as text_file:\n",
    "    oliver_text = text_file.read()\n",
    "oliver_top_terms = freq_normed_unigrams(tokenize_and_tag_text(oliver_text), 50)\n",
    "\n",
    "\n",
    "print(\"PRINTING UNIGRAMS WITH HYPERNYMS FOR OLIVER TWIST CORPUS...\\n\")\n",
    "categories_from_hypernyms(oliver_top_terms,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking using Noun Phrases with prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PRINTING GIST OF BROWN NEWS CORPUS USING NOUN PHRASE CHUNKING... \n",
      "\n",
      "  number ,  years ,  kind ,  board ,  government ,  work ,  state ,  interest ,  use ,  education ,  service ,  program ,  Communist ,  President ,  game ,  members ,  libraries ,  sense ,  group ,  member ,  series ,  mind ,  labor ,  golf ,  need ,  front ,  charge ,  time ,  schools ,  plan ,  aid ,  machinery ,  director ,  hand ,  bonds ,  center ,  race ,  faculty ,  headquarters ,  Sen. ,  health ,  price ,  chairman ,  problem ,  election ,  day ,  State ,  cost ,  lot ,  base ,  farm ,  men ,  governor ,  hundreds ,  administration ,  acts ,  behalf ,  directors ,  excellence ,  lines ,  increase ,  thousands ,  conspiracy ,  couple ,  family ,  art ,  form ,  fire ,  discrimination ,  that ,  methods ,  history ,  Hill ,  control ,  months ,  grants ,  Capitol ,  side ,  families ,  traffic ,  sales ,  Mayor ,  population ,  office ,  Judge ,  field ,  prison ,  degree ,  trade ,  pair ,  support ,  construction ,  nations ,  employment ,  salary ,  research ,  records ,  reports ,  employes ,  school ,  hours ,  baseball ,  home ,  states ,  business ,  balls ,  public ,  cent ,  library ,  land ,  evening ,  man ,  city's ,  institutions ,  year's ,  help ,  sort ,  services ,  World ,  area ,  millions ,  dollars ,  cash ,  worth ,  candidate ,  bill ,  cotton ,  order ,  honor ,  source ,  troops ,  wisdom ,  tax ,  income ,  unions ,  wages ,  roads ,  result ,  owners ,  plane ,  women ,  $5,000 ,  town ,  agreement ,  types ,  demand ,  Government ,  Day ,  gin ,  world ,  revenues ,  drop ,  persons ,  participation ,  total ,  act ,  earnings ,  growth ,  trouble ,  trustees ,  publicity ,  victory ,  ADC ,  Gov. ,  this ,  convenience ,  property ,  basis ,  relations ,  assault ,  shortage ,  return ,  committee ,  concerts ,  dates ,  year ,  bills ,  single ,  variety ,  theater ,  night ,  record ,  graduate ,  peace ,  establishment ,  head ,  funds ,  Premier ,  period ,  meeting ,  money ,  Hospital ,  law ,  community ,  film ,  yards ,  collection ,  air ,  sale ,  fees ,  independence ,  society ,  War ,  city ,  confidence ,  gown ,  Dr. ,  care ,  equipment ,  trend ,  opinion ,  measure ,  policy ,  science ,  combination ,  endowments ,  existence ,  housing ,  hits ,  bit ,  panels ,  justice ,  children ,  prospects ,  taxes ,  banks ,  forces ,  sheriff ,  scholarship ,  payments ,  bases ,  expenses ,  lots ,  dismissal ,  party ,  encouragement ,  check ,  Coach ,  $1 ,  requirements ,  expansion ,  exhibition ,  contributions ,  errors ,  decade ,  set ,  investigation ,  teams ,  lack ,  Senators ,  student ,  Secretary ,  performances ,  This ,  Gen. ,  pay ,  square ,  force ,  round ,  integration ,  county ,  improvement ,  Aj ,  Palm ,  products ,  satisfaction ,  marriage ,  runs ,  information ,  feet ,  investors ,  thing ,  point ,  Grove ,  philosophy ,  freedom ,  pedestrians ,  charter ,  strength ,  favor ,  President-elect ,  system ,  selection ,  candidates ,  burns ,  laws ,  conferences ,  college ,  outlook ,  majority ,  canvassers ,  rule ,  possibility ,  supporters ,  toll-road ,  textiles ,  address ,  corn ,  Communism ,  age ,  hill ,  bags ,  humor ,  Shamrock ,  food ,  homes ,  houses ,  session ,  position ,  $2 ,  war ,  rate ,  Union ,  designs ,  rifles ,  countries ,  Beach ,  $50 ,  car ,  stock ,  superintendent ,  net ,  paternalism ,  difference ,  district ,  fly ,  knowledge ,  workers ,  hospital ,  end ,  Avenue ,  circumstances ,  union ,  distribution ,  step ,  $12.50 ,  activities ,  woods ,  flow ,  defense ,  interviews ,  consumer ,  wardens ,  acre ,  filling ,  rise ,  maid ,  causes ,  changes ,  recommendations ,  pressure ,  resumption ,  confusion ,  times ,  fight ,  shares ,  masses ,  hands ,  letters ,  pension ,  core ,  miles ,  battle ,  emergency ,  deliberation ,  hazards ,  satin ,  television ,  goods ,  questions ,  policies ,  humans ,  team ,  reference ,  connection ,  earrings ,  desegregation ,  St. ,  toll ,  prize ,  place ,  beauty ,  top ,  experience ,  choice ,  States ,  medicine ,  matter ,  tongues ,  divorce ,  firms ,  negotiations ,  pickup ,  NATO ,  citizens ,  tournaments ,  efforts ,  snow ,  court ,  word ,"
     ]
    }
   ],
   "source": [
    "def chunker(sentence_list):\n",
    "    cp = nltk.RegexpParser(\"CHUNK: {<DT>?<NN.*>+<IN><NN.*>+}\")  # Noun phrases, ending with nouns including prepositions\n",
    "    list_words = []\n",
    "    for sent_no in range(len(sentence_list)):\n",
    "        tree = cp.parse(sentence_list[sent_no])\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'CHUNK' and len(subtree) > 2: # retaining chunks with 3 or more words\n",
    "                for word, tag in subtree.leaves():\n",
    "                    if tag != 'IN':     # Excluding preposition from found chunks to get frequently gropued words\n",
    "                        list_words.append(word)\n",
    "    fd = nltk.FreqDist(list_words)\n",
    "    most_common = fd.most_common(400)\n",
    "    return most_common\n",
    "\n",
    "\n",
    "# Applying Chunking Brown Corpus \n",
    "\n",
    "brown = nltk.corpus.brown\n",
    "brown_sentence_list = brown.tagged_sents(categories = 'news')\n",
    "\n",
    "brown_most_common = chunker(brown_sentence_list)\n",
    "\n",
    "print(\" PRINTING GIST OF BROWN NEWS CORPUS USING NOUN PHRASE CHUNKING... \\n\",)\n",
    "\n",
    "for word, freq in brown_most_common:\n",
    "    print(' ', word ,',', end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Oliver_Copy.txt', 'r') as text_file:\n",
    "    oliver_corpus = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PRINTING GIST OF OLIVER CORPUS USING NOUN PHRASE CHUNKING... \n",
      "\n",
      "  's ,  oliver ,  mr. ,  time ,  this ,  bumble ,  sikes ,  fagin ,  life ,  hand ,  look ,  head ,  way ,  couple ,  london ,  place ,  day ,  tears ,  mrs. ,  piece ,  bread ,  house ,  death ,  deal ,  man ,  face ,  side ,  course ,  expression ,  glass ,  pair ,  money ,  night ,  gentleman ,  business ,  sort ,  i ,  paper ,  brownlow ,  air ,  men ,  front ,  thing ,  hour ,  number ,  cry ,  nancy ,  fear ,  state ,  heart ,  home ,  spirits ,  toby ,  boy ,  question ,  mind ,  scrap ,  water ,  society ,  manner ,  street ,  foot ,  help ,  sound ,  noise ,  fire ,  point ,  flight ,  another ,  corney ,  crackit ,  bates ,  charley ,  terror ,  length ,  morning ,  alarm ,  that ,  company ,  exclamations ,  view ,  show ,  noah ,  loss ,  heaven ,  voice ,  office ,  eyes ,  love ,  breath ,  reply ,  ale ,  name ,  action ,  surprise ,  bit ,  word ,  passion ,  order ,  form ,  chitling ,  agony ,  thieves ,  flash ,  burst ,  faces ,  stairs ,  maylie ,  intelligence ,  room ,  happiness ,  st. ,  tone ,  sense ,  clothes ,  presence ,  quantity ,  hair ,  lane ,  blood ,  year ,  laughter ,  turn ,  things ,  variety ,  joy ,  part ,  'the ,  matter ,  bed ,  master ,  power ,  sight ,  beer ,  nature ,  matters ,  escape ,  silence ,  slice ,  mode ,  advance ,  children ,  smoke ,  dogs ,  effect ,  scores ,  exclamation ,  sowerberry ,  suit ,  glasses ,  birth ,  cup ,  amazement ,  grimwig ,  command ,  impatience ,  justice ,  want ,  leave ,  history ,  friend ,  benefit ,  fragments ,  hours ,  direction ,  crowd ,  pane ,  drop ,  cluster ,  humanity ,  position ,  road ,  liquor ,  jew ,  space ,  gruel ,  conversation ,  pleasure ,  hope ,  talk ,  food ,  field ,  astonishment ,  peace ,  god ,  pieces ,  mass ,  work ,  words ,  furniture ,  chertsey ,  sleep ,  train ,  midst ,  habit ,  notice ,  care ,  case ,  violence ,  spirit ,  others ,  back ,  voices ,  neck ,  marks ,  boys ,  cunning ,  dinner ,  mann ,  breakfast ,  town ,  sickness ,  week ,  'a ,  cause ,  monks ,  pint ,  pain ,  game ,  heaps ,  distance ,  cloud ,  safety ,  basin ,  set ,  endeavouring ,  sympathy ,  paroxysm ,  noon ,  fever ,  affairs ,  legs ,  bill ,  mercy ,  shelter ,  tie ,  dust ,  traces ,  charlotte ,  speech ,  church ,  angel ,  features ,  attempt ,  cheese ,  women ,  hearts ,  horseback ,  oliver's ,  coat ,  awe ,  force ,  hate ,  'not ,  subject ,  paces ,  tea ,  kind ,  sign ,  pound ,  peal ,  world ,  claypole ,  persons ,  object ,  hill ,  property ,  concealment ,  houses ,  destination ,  person ,  flood ,  light ,  discourse ,  interval ,  years ,  flow ,  degree ,  draught ,  passengers ,  drops ,  meat ,  articles ,  custody ,  delight ,  pentonville ,  acknowledgment ,  parchment ,  experience ,  youth ,  roar ,  gleam ,  islington ,  clusters ,  treatment ,  eye ,  n't ,  shoes ,  people ,  limbkins ,  reason ,  line ,  friends ,  bunch ,  streets ,  assistance ,  supply ,  'm ,  dress ,  keys ,  hands ,  midnight ,  duty ,  consideration ,  dislike ,  mouth ,  blow ,  hundreds ,  moment ,  timber ,  lightning ,  countenance ,  showers ,  address ,  gin-and-water ,  tokens ,  window ,  correction ,  expense ,  genius ,  crown ,  evening ,  amusement ,  complacency ,  dignity ,  mug ,  behalf ,  sacks ,  england ,  residence ,  desperation ,  symptoms ,  handkerchiefs ,  sorts ,  drink ,  need ,  quarter ,  sounds ,  ashy ,  fears ,  earth ,  run ,  argument ,  irrepressible ,  second-hand ,  prayer ,  dodger ,  trampling ,  newgate ,  pot ,  energy ,  thought ,  pocket ,  call ,  warehouses ,  crib ,  disclosure ,  discovery ,  beginning ,  pay ,  motion ,  art ,  custom ,  chinking ,  lad ,  neighborhood ,  hold ,  thunder ,  books ,  hatfield ,  pitch ,  orange-peel ,  delay ,  impulses ,  press ,  rights ,  companion ,  station ,  firm ,  sorrow ,  arms ,  answer ,  circle ,  result ,  majesty ,  mother ,  style ,  strength ,  paleness ,  spectacles ,"
     ]
    }
   ],
   "source": [
    "oliver_tagged_sent = tokenize_and_tag_text(oliver_corpus)\n",
    "oliver_most_common = chunker(oliver_tagged_sent)\n",
    "\n",
    "#Chunking Oliver Twist corpus\n",
    "print(\" PRINTING GIST OF OLIVER CORPUS USING NOUN PHRASE CHUNKING... \\n\")\n",
    "\n",
    "for word, freq in oliver_most_common:\n",
    "    print(' ', word ,',', end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations-Brown News Corpus\n",
    " 1. Finding trigrams that occur at least 2 times, retain top 100 according to PMI\n",
    " 2. Finding bigrams that occur at least 4 times, which are not subsumed by the retained trigrams, ordering by PMI, retaining top 100\n",
    " 3. Excluding trigrams that begin or end with stopwords, removing stopwords from bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING GIST OF COLLOCATIONS FOR BROWN NEWS CORPUS.... \n",
      " \n",
      "  Ku Klux Klan ,  Pinar Del Rio ,  Rural Roads Authority ,  Post Office Box ,  Diversified Growth Stock ,  Patrick's Day Purse ,  Notre Dame Chapter ,  esprit de corps ,  Growth Stock Fund ,  electronic data processing ,  La Dolce Vita ,  oil mill supplies ,  Prince Souvanna Phouma ,  Holy Cross Hospital ,  First Lady Jacqueline ,  Duncan Phyfe furniture ,  Cherry Hill Road ,  Dow Jones industrial ,  Speaker Sam Rayburn ,  J. Clinton Bowman ,  Christian Youth Crusade ,  Stage 1 Residential ,  Big Four summit ,  15 pounds lighter ,  Mile Road East ,  Robert O. Spurdle ,  Philmont Country Club ,  test ban negotiations ,  Gin Supply Co. ,  potato chip industry ,  South Viet Nam ,  $800 billion economy ,  Air Force Academy ,  Armed Services Committee ,  Emory University's charter ,  nuclear test ban ,  Ivan Allen Jr. ,  fire fighters association ,  T. F. Zimmerman ,  Henry Hall Wilson ,  South Park Way ,  total passing yardage ,  cotton ginning machinery ,  Atty. Gen. J. ,  Highway Department source ,  National Audubon Society ,  First Presbyterian Church ,  Labor Relations Board ,  Henry L. Bowden ,  Anne Arundel county ,  North Viet Nam ,  Lady Jacqueline Kennedy ,  real estate agent ,  St. Patrick's Day ,  matching fund basis ,  public safety commissioner ,  Senate Foreign Relations ,  gross national product ,  data processing system ,  Democratic gubernatorial nomination ,  Carl W. Buchheister ,  World War 2 ,  Morton Foods stock ,  Foreign Relations Committee ,  state's occupation tax ,  Blue Island Av. ,  Christian Family Week ,  J. Joseph Nugent ,  Jones industrial average ,  Fulton Superior Court ,  set freight rates ,  Central Falls City ,  aged care plan ,  W. H. Roquemore ,  Sen. George Parkhouse ,  left front wheel ,  strokes behind Player ,  Falls City Council ,  John P. Figone ,  executions took place ,  rural roads bonds ,  American Friends Service ,  National Football League ,  social security payroll ,  Rev. T. F. ,  Rhode Island Hospital ,  West Pratt Street ,  National Maintenance company ,  New Eastwick Corp. ,  running mates without ,  G. David Thompson ,  merely paid employees ,  April 4 ballot ,  First Christian Church ,  Junior Achievement program ,  gas station man ,  Foods stock issue ,  four strokes behind ,  12 months ending ,  2% sales tax ,  Scottish Rite ,  Thrift Shop ,  Hong Kong ,  Pathet Lao ,  Citizens Group ,  23d ward ,  Patrice Lumumba ,  Greer Garson ,  Beverly Hills ,  collective bargaining ,  International Airport ,  Los Angeles ,  Southeast Asia ,  General Assembly ,  civil servants ,  wedding trip ,  Latin America ,  San Francisco ,  Southern California ,  Attorney General ,  Pennsylvania Avenue ,  anti-monopoly laws ,  High School ,  civil defense ,  Premier Khrushchev ,  $28 million ,  $60 million ,  anti-trust laws ,  higher learning ,  Dr. Clark ,  Dr. Jenkins ,  dental schools ,  Soviet Union ,  White Sox ,  vice president ,  United States ,  per cent ,  United Nations ,  full amount ,  farm equipment ,  religious community ,  began operations ,  Soviet leader ,  coalition government ,  grand jury ,  million dollars ,  White House ,  Arnold Palmer ,  higher education ,  Miss Garson ,  weeks ago ,  federal grants ,  local level ,  Mickey Mantle ,  school superintendent ,  Judge Smith ,  Catholic higher ,  car driven ,  Miss Mary ,  home runs ,  million worth ,  member libraries ,  President Kennedy's ,  U.S. coal ,  faculty members ,  State College ,  years ago ,  special session ,  high school ,  York Yankees ,  former Miss ,  home rule ,  told police ,  tomorrow night ,  high schools ,  p.m. Sunday ,  three games ,  Friday night ,  little car ,  Saturday night ,  three hours ,  Tuesday night ,  year earlier ,  home run ,  Monday night ,  last night ,  two weeks ,  last year ,  Washington State ,  year ago ,  two days ,  last Saturday ,  would like ,  three years ,  would increase ,  next two ,  last season ,  two years ,  two men ,  one time ,"
     ]
    }
   ],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(brown.words(categories = 'news'))\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "finder.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: w.lower() in my_stopwords)\n",
    "\n",
    "\n",
    "trigram_list = finder.nbest(trigram_measures.pmi, 100)\n",
    "    \n",
    "trigram_set = set()\n",
    "for a,b,c in trigram_list:\n",
    "    trigram_set.add(a.lower())\n",
    "    trigram_set.add(b.lower())\n",
    "    trigram_set.add(c.lower())\n",
    "\n",
    "        \n",
    "    \n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(brown.words(categories = 'news'))\n",
    "\n",
    "finder.apply_freq_filter(4)\n",
    "finder.apply_word_filter(lambda w: w.lower() in trigram_set)\n",
    "finder.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in my_stopwords)\n",
    "bigram_list = finder.nbest(bigram_measures.pmi, 100)\n",
    " \n",
    "    \n",
    "print('PRINTING GIST OF COLLOCATIONS FOR BROWN NEWS CORPUS.... \\n ')\n",
    "\n",
    "for a, b, c in trigram_list:\n",
    "    print(' ', a, b, c, ',', end=\"\",)\n",
    "for a, b in bigram_list:\n",
    "    print(' ', a, b, ',', end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations-Oliver Twist\n",
    "  1. Finding trigrams that occur at least 2 times, retain top 100 according to PMI\n",
    "  2. Finding bigrams that occur at least 4 times, which are not subsumed by the retained trigrams, ordering by PMI, retaining top 100\n",
    "  3. Excluding trigrams that begin or end with stopwords, after removing stopwords from bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Oliver_Copy.txt', 'r') as text_file:\n",
    "    oliver_corpus = text_file.read()\n",
    "    \n",
    "oliver_tokens=nltk.word_tokenize(oliver_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "more_stopwords = [\"''\", \"--\",\"``\", \"mr.\", \"mrs.\", \"n't\", \"'s\", \"'i\",\"said\"]\n",
    "my_stopwords = stop_words + more_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING GIST OF COLLOCATIONS FOR OLIVER TWIST.... \n",
      " \n",
      "  large brass buttons ,  Little Saffron Hill ,  sun shone brightly ,  regular right-down bad ,  right-down bad 'un ,  d 'ye mean ,  D 'ye hear ,  shouting 'Stop thief ,  chairs closer together ,  flash Toby Crackit ,  soft blue eye ,  Master Charles Bates ,  crept softly upstairs ,  hardened little wretch ,  coach rattled away ,  three pound ten ,  sends back word ,  Come home directly ,  murdered woman lay ,  name engraved upon ,  'A porochial life ,  'You may depend ,  'Never say die ,  girl drew closer ,  night afore last ,  may depend upon ,  added Charley Bates ,  white-headed old gentleman ,  turning quickly round ,  Master Charley Bates ,  'We shall see ,  great deal better ,  Oliver lay awake ,  merry old gentleman ,  absent old gentleman ,  inquired Charley Bates ,  last two days ,  yer know yer ,  went away together ,  Oliver felt glad ,  'You 're right ,  replied Master Bates ,  face turned towards ,  'We must know ,  cried Charley Bates ,  eyes fixed upon ,  great many times ,  dear old nurse ,  without another word ,  two old gentlemen ,  shall never get ,  never get another ,  make 'em come ,  two young ladies ,  hideous old man ,  must go back ,  two old women ,  still came back ,  poor Oliver Twist ,  second old gentleman ,  replied Sikes impatiently ,  'You may say ,  old gentleman laughed ,  heavy hand upon ,  kind old lady ,  young Oliver Twist ,  dear young lady ,  inquired poor Oliver ,  dear young friend ,  take one look ,  old gentleman gave ,  young gentleman took ,  'The old gentleman ,  poor little Oliver ,  old lady made ,  old gentleman looked ,  Oliver could see ,  Clerkinwell Sessions ,  book-stall keeper ,  sitting posture ,  Jack Dawkins ,  Messrs. Blathers ,  Conkey Chickweed ,  street-door key ,  eleven o'clock ,  Morris Bolter ,  nodded assent ,  shortly afterwards ,  six weeks ,  cocked hat ,  God bless ,  'Poor fellow ,  'Of course ,  new comer ,  five pounds ,  white waistcoat ,  gold watch ,  rapid pace ,  upper end ,  tremulous voice ,  five minutes ,  'Now listen ,  twelve years ,  hours ago ,  presently returned ,  remained silent ,  dim light ,  anxious faces ,  short pause ,  Miss Maylie ,  caught sight ,  fell asleep ,  to-morrow morning ,  Artful Dodger ,  stopping short ,  Miss Nancy ,  years ago ,  Harry Maylie ,  gone mad ,  either side ,  well acquainted ,  Noah Claypole ,  've got ,  next morning ,  heart beat ,  sooner heard ,  opposite side ,  mere child ,  quite certain ,  anything else ,  nothing else ,  low voice ,  short distance ,  'Oh yes ,  good humour ,  new friends ,  pretty well ,  high chair ,  next day ,  long ago ,  'll eat ,  short silence ,  'He says ,  'Very good ,  Miss Rose ,  strange place ,  every direction ,  full length ,  passed along ,  've found ,  demanded Monks ,  left alone ,  've seen ,  strong enough ,  asked Monks ,  would seem ,  quite dark ,  Rose Maylie ,  next room ,  long silence ,  Jew nodded ,  'll give ,  strange boy ,  quite enough ,  every day ,  asked Noah ,  Fagin nodded ,  heard enough ,  long time ,  'll soon ,  'll tell ,  'll keep ,  quite well ,  short time ,  rejoined Rose ,  door behind ,  asked Rose ,"
     ]
    }
   ],
   "source": [
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(oliver_tokens)\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "\n",
    "finder.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: w.lower() in my_stopwords)\n",
    "\n",
    "\n",
    "trigram_list = finder.nbest(trigram_measures.pmi, 100)\n",
    "\n",
    "    \n",
    "trigram_set = set()\n",
    "for a,b,c in trigram_list:\n",
    "    trigram_set.add(a.lower())\n",
    "    trigram_set.add(b.lower())\n",
    "    trigram_set.add(c.lower())\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(oliver_tokens)\n",
    "\n",
    "finder.apply_freq_filter(4)\n",
    "finder.apply_word_filter(lambda w: w.lower() in trigram_set)\n",
    "finder.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in my_stopwords)\n",
    "bigram_list = finder.nbest(bigram_measures.pmi, 100)\n",
    "\n",
    "    \n",
    "print('PRINTING GIST OF COLLOCATIONS FOR OLIVER TWIST.... \\n ')\n",
    "\n",
    "for a, b, c in trigram_list:\n",
    "    print(' ', a, b, c, ',', end=\"\",)\n",
    "for a, b in bigram_list:\n",
    "    print(' ', a, b, ',', end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
